{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "Python version\n",
      "3.6.9 (default, Oct  8 2020, 12:12:24) \n",
      "[GCC 8.4.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "print('Python version')\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = cifar100.load_data(label_mode=\"fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainy.shape,testX.shape,testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71,  8, 97,\n",
       "        80, 74, 59, 70, 87, 84, 64, 52, 42, 47, 65, 21, 22, 81, 24, 78, 45,\n",
       "        49, 56, 76, 89, 73, 14,  9,  6, 20, 98, 36, 55, 72, 43, 51, 35, 83,\n",
       "        33, 27, 53, 92, 50, 15, 18, 46, 75, 38, 66, 77, 69, 95, 99, 93,  4,\n",
       "        61, 94, 68, 34, 32, 88, 67, 30, 62, 63, 40, 26, 48, 79, 85, 54, 44,\n",
       "         7, 12,  2, 41, 37, 13, 25, 10, 57,  5, 60, 91,  3, 58, 16]),\n",
       " 95    500\n",
       " 74    500\n",
       " 39    500\n",
       " 71    500\n",
       " 8     500\n",
       "      ... \n",
       " 85    500\n",
       " 22    500\n",
       " 54    500\n",
       " 86    500\n",
       " 0     500\n",
       " Length: 100, dtype: int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info on y labels. \n",
    "trainy_label=pd.Series(trainy.ravel())\n",
    "trainy_label.unique(),trainy_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out three classes out of 100\n",
    "def sep_class(trainx,trainy,y):\n",
    "    class_x=[]\n",
    "    for i in range(len(trainx)):\n",
    "        if trainy[i]==y:\n",
    "           #Normalise by 255\n",
    "            class_x.append(trainx[i]/255)\n",
    "        y_label=y*np.ones((len(class_x),1))\n",
    "    return (np.array(class_x),np.array(y_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 32, 32, 3), (1800, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separate out three classes out of 100   \n",
    "trainX0,trainy0=sep_class(trainX,trainy,0)\n",
    "testX0,testy0=sep_class(testX,testy,0)\n",
    "\n",
    "#Separate out one classes out of 100   \n",
    "trainX1,trainy1=sep_class(trainX,trainy,1)\n",
    "testX1,testy1=sep_class(testX,testy,1)\n",
    "\n",
    "#Separate out 2 class out of 100   \n",
    "trainX2,trainy2=sep_class(trainX,trainy,2)\n",
    "testX2,testy2=sep_class(testX,testy,2)\n",
    "\n",
    "# ----combining training and test data together---\n",
    "dataX=np.concatenate((trainX0,testX0,trainX1,testX1,trainX2,testX2)) # concatenating both train and test togther\n",
    "datay=np.concatenate((trainy0,testy0,trainy1,testy1,trainy2,testy2))\n",
    "\n",
    "# training samples\n",
    "dataXshuffle=list(zip(dataX,datay))\n",
    "np.random.shuffle(dataXshuffle)\n",
    "\n",
    "dataXn,datayn=zip(*dataXshuffle)\n",
    "dataXn=np.array(dataXn)\n",
    "datayn=np.array(datayn)\n",
    "dataXn.shape, datayn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1620, 32, 32, 3) (180, 32, 32, 3)\n",
      "(1620, 3072) (180, 3072) (1620, 1) (180, 1)\n"
     ]
    }
   ],
   "source": [
    "#split data into training and test set\n",
    "trainXt,testXt,trainy,testy = train_test_split(dataXn,datayn,test_size=0.1,random_state=42)\n",
    "print(trainXt.shape,testXt.shape)\n",
    "trainX=trainXt.reshape(-1,32*32*3)\n",
    "testX=testXt.reshape(-1,32*32*3)\n",
    "print(trainX.shape,testX.shape, trainy.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy=keras.utils.to_categorical(trainy,3)\n",
    "testy=keras.utils.to_categorical(testy,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((list(testX),list(testy))).batch(len(testy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=3, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    #clientN={'client_names[0]': shards[0]}\n",
    "    #return clientN\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['client_1', 'client_2', 'client_3'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create clients\n",
    "clients = create_clients(trainX,trainy, num_clients=3, initial='client')\n",
    "clients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: client_1 540\n",
      "key: client_2 540\n",
      "key: client_3 540\n"
     ]
    }
   ],
   "source": [
    "for key, value in clients.items():\n",
    "    print('key:',key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=24):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('client_1', <BatchDataset shapes: ((None, 3072), (None, 3)), types: (tf.float64, tf.float32)>), ('client_2', <BatchDataset shapes: ((None, 3072), (None, 3)), types: (tf.float64, tf.float32)>), ('client_3', <BatchDataset shapes: ((None, 3072), (None, 3)), types: (tf.float64, tf.float32)>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_batched.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 3072), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_batched['client_1'].element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(500, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(300))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 \n",
    "comms_round = 25\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)        \n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 65.000% | global_loss: 0.9423385858535767\n",
      "comm_round: 1 | global_acc: 70.556% | global_loss: 0.8979196548461914\n",
      "comm_round: 2 | global_acc: 70.000% | global_loss: 0.8777601718902588\n",
      "comm_round: 3 | global_acc: 70.000% | global_loss: 0.8953971862792969\n",
      "comm_round: 4 | global_acc: 72.778% | global_loss: 0.8568248152732849\n",
      "comm_round: 5 | global_acc: 66.667% | global_loss: 0.8861738443374634\n",
      "comm_round: 6 | global_acc: 71.111% | global_loss: 0.8562011122703552\n",
      "comm_round: 7 | global_acc: 75.000% | global_loss: 0.8331377506256104\n",
      "comm_round: 8 | global_acc: 74.444% | global_loss: 0.8322610855102539\n",
      "comm_round: 9 | global_acc: 76.667% | global_loss: 0.8255817890167236\n",
      "comm_round: 10 | global_acc: 73.889% | global_loss: 0.83018559217453\n",
      "comm_round: 11 | global_acc: 75.000% | global_loss: 0.819553792476654\n",
      "comm_round: 12 | global_acc: 75.000% | global_loss: 0.8259878158569336\n",
      "comm_round: 13 | global_acc: 78.889% | global_loss: 0.8108406662940979\n",
      "comm_round: 14 | global_acc: 79.444% | global_loss: 0.7958610653877258\n",
      "comm_round: 15 | global_acc: 77.222% | global_loss: 0.8108982443809509\n",
      "comm_round: 16 | global_acc: 78.889% | global_loss: 0.796260416507721\n",
      "comm_round: 17 | global_acc: 71.667% | global_loss: 0.8381554484367371\n",
      "comm_round: 18 | global_acc: 76.111% | global_loss: 0.8139433264732361\n",
      "comm_round: 19 | global_acc: 76.111% | global_loss: 0.8077951073646545\n",
      "comm_round: 20 | global_acc: 76.667% | global_loss: 0.8176684975624084\n",
      "comm_round: 21 | global_acc: 79.444% | global_loss: 0.7877764105796814\n",
      "comm_round: 22 | global_acc: 80.000% | global_loss: 0.7788490056991577\n",
      "comm_round: 23 | global_acc: 76.111% | global_loss: 0.7982788681983948\n",
      "comm_round: 24 | global_acc: 76.111% | global_loss: 0.783397912979126\n"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(32*32*3, 3)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    #print(len(global_weights))\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    #print(client_names)\n",
    "    np.random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(32*32*3, 3)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        from tensorflow.keras import backend as K\n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
